"""LLM Safety Testing Framework.

A modular framework for testing LLM safety systems against adversarial prompts.
"""

__version__ = "1.0.0"
__author__ = "Taylor Amarel"

__all__ = [
    "__version__",
    "__author__",
]
